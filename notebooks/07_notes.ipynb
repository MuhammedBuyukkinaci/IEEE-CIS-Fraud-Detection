{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "np.set_printoptions(suppress=True)\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import decimal\n",
    "from random import shuffle\n",
    "from time import sleep\n",
    "tqdm.pandas()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(suppress=True)\n",
    "import random\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always seed the randomness of this universe\n",
    "def seed_everything(seed=51):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n",
      "CPU times: user 38.4 s, sys: 3.35 s, total: 41.7 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/train4.csv')\n",
    "print(\"train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test finished\n",
      "CPU times: user 32.4 s, sys: 2.52 s, total: 35 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv('../input/test4.csv')\n",
    "print(\"test finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 805) (506691, 804) (506691, 2)\n"
     ]
    }
   ],
   "source": [
    "#sample_submission\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(train.shape,test.shape,sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 804) (506691, 804)\n"
     ]
    }
   ],
   "source": [
    "y = train['isFraud']\n",
    "del train['isFraud']\n",
    "gc.collect()\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODE_BITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility: encode binary 0/1 columns as bits in a single integer\n",
    "def encode_bits(binary_df):\n",
    "    ncols = binary_df.shape[1]\n",
    "    assert ncols < 64\n",
    "    return binary_df @ (1 << np.arange(ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    4,    8,   16,   32,   64,  128,  256,  512, 1024,\n",
       "       2048, 4096, 8192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 << np.arange(14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in train.columns if x[0] =='C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1   C2   C3   C4   C5   C6   C7   C8   C9  C10  C11  C12   C13  C14\n",
       "0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  2.0  0.0   1.0  1.0\n",
       "1  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   1.0  1.0\n",
       "2  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0   1.0  1.0\n",
       "3  2.0  5.0  0.0  0.0  0.0  4.0  0.0  0.0  1.0  0.0  1.0  0.0  25.0  1.0\n",
       "4  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0   1.0  1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[x for x in train.columns if x[0] =='C']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = encode_bits(train[[x for x in train.columns if x[0] =='C']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     14627.0\n",
       "1     13347.0\n",
       "2     13603.0\n",
       "3    112012.0\n",
       "4     13987.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_count = train.columns[2:].tolist()\n",
    "\n",
    "for c in to_count:\n",
    "    s = train[c]\n",
    "    if hasattr(s, 'cat'):\n",
    "        s = s.cat.codes\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    train[f'{c}_count'] = s.map(vc).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran['TimeInDay'] = tran.TransactionDT % 86400\n",
    "tran['Cents'] = tran.TransactionAmt % 1\n",
    "tran['C_bin'] = encode_bits(tran[CCOLS]>0)\n",
    "tran['D_bin'] = encode_bits(tran[DCOLS].isnull())\n",
    "tran['M_bin'] = encode_bits(tran[MCOLS].isnull())\n",
    "tran['addr_bin'] = encode_bits(tran[['addr1','addr2','dist1','dist2']].isnull())\n",
    "tran['email_bin'] = encode_bits(tran[['R_emaildomain','P_emaildomain']].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hlin117/mdlp-discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_use = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'C1', 'C3', 'C5', 'C13', 'D1', 'D3', 'D4', 'D10',\n",
    "                  'D15', 'V12', 'V14', 'V15', 'V19', 'V29', 'V35', 'V37', 'V39', 'V41', 'V48', 'V53', 'V55', 'V56', 'V61', 'V75', 'V77', \n",
    "                  'V78', 'V79', 'V80', 'V82', 'V86', 'V88', 'V95', 'V98', 'V99', 'V100', 'V104', 'V107', 'V108', 'V109', 'V110', 'V111',\n",
    "                  'V112', 'V114', 'V115', 'V116', 'V117', 'V118', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V129', 'V130', 'V131', \n",
    "                  'V135', 'V136', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V300', 'V303', \n",
    "                  'V305', 'V310', 'V311', 'V312', 'V313', 'V314', 'V319', 'V320', 'Transaction_dow', 'Transaction_hour', 'M6', 'card4', \n",
    "                  'card6', 'P_emaildomain', 'ProductCD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null number Equals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group1'] = ['D1', 'V281', 'V282', 'V283', 'V288', 'V289', 'V296', 'V300', 'V301', 'V313', 'V314', 'V315']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group2'] = ['D11', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group3'] = ['M1', 'M2', 'M3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group4'] = ['M8', 'M9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group5'] = ['id_01', 'id_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group6'] = ['id_15', 'id_35', 'id_36', 'id_37', 'id_38','id_11', 'id_28', 'id_29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group7'] = ['id_05', 'id_06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group8'] = ['D8', 'D9', 'id_09', 'id_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_equal['group9'] = ['id_03', 'id_04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My guess is that D1/D2 are about \"how many days have passed from the first transaction\", \n",
    "D3 is responsible for \"how many days have passed from the previous transaction\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is not correct. D9 is hour/24. You can also say that it is hours passed\n",
    "since transaction. D8 is days passed since last transaction and D9 is its decimal part.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Any ordered categorical variables we've already described as numerical \n",
    "(e.g. Cx, Dx, id-1 to id-11, etc), anything else are non-ordering categorical\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "id_14 Correct, it's timezone,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Just adding a feature which is the merger of card1,2,3 and 4\n",
    " and then encoded improves model accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Thanks Chris! I actually did some extra work, thanks to the great topic from @snovik1975\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/discussion/107791#latest-622119\n",
    "\n",
    "The missing values actually are highly related to different ProductCD.\n",
    "For example, if we look at V1 - V11, the missing values from W is 29.21%\n",
    "However, for H/C/S/R, 100% are missing!\n",
    "\n",
    "As you can check for other blocks, similar story will happen. Looks like the ProductCD is a key.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "V-columns have lots of time-based structure in them and there are also \n",
    "non-linear interrelations among them. Any PCA-style dimensionality reduction\n",
    "is at least pointless and quite probably harmful. And even worse - those features \n",
    "start their life before the beginning of the train set therefore they might \n",
    "have values which we can figure out from the train but also those which we cannot.\n",
    "And this is not on the feature level but within the feature.\n",
    "So go figure out why organizers made it so complex with these pre-calculated features\n",
    "Puzzle makes me love and hate it, LOL. I'm now trying to split by ProductCD, \n",
    "and filter some columns first.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "So if categorical feature is given as numerical, LigthGBM could miss to extract full potential \n",
    "information from it. On the other hand, if given as categorical, \n",
    "there is bigger risk of overfitting.\n",
    "From what I've observed, it could be a good idea to try specifying high-cardinality\n",
    "categorical features as numeric (as they typically have high overfitting risk).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Permutation importance for solo features\n",
    "Recursive feature elimination for block of features\n",
    "PCA for groups of identical features (V columns)\n",
    "-- Didn't check adversarial validation (next thing to do)\n",
    "\n",
    "As I see all approaches work. And features selection is one of the key points to boost score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['V1', 'V10', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V12', 'V120',\n",
    "'V121', 'V122', 'V123', 'V124', 'V125', 'V13', 'V130', 'V131', 'V135', 'V136', 'V137', 'V138', 'V139', 'V14', 'V140', 'V141', 'V142',\n",
    "'V146', 'V147', 'V149', 'V152', 'V154', 'V158', 'V159', 'V161', 'V162', 'V165', 'V166', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174',\n",
    "'V175', 'V176', 'V18', 'V180', 'V181', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V19', 'V190', 'V194', 'V195', 'V197',\n",
    "'V198', 'V199', 'V2', 'V20', 'V200', 'V201', 'V205', 'V207', 'V208', 'V209', 'V210', 'V216', 'V22', 'V220', 'V221', 'V223', 'V224',\n",
    "'V226', 'V227', 'V228', 'V229', 'V23', 'V230', 'V234', 'V235', 'V238', 'V239', 'V24', 'V240', 'V241', 'V242', 'V243', 'V245', 'V246',\n",
    "'V247', 'V25', 'V250', 'V252', 'V253', 'V255', 'V257', 'V258', 'V259', 'V26', 'V260', 'V261', 'V262', 'V263', 'V264', 'V267', 'V268',\n",
    "'V27', 'V271', 'V274', 'V277', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V292', 'V297', 'V3', \n",
    "'V30', 'V300', 'V301', 'V302', 'V303', 'V305', 'V309', 'V310', 'V312', 'V313', 'V314', 'V315', 'V319', 'V320', 'V321', 'V325', 'V334',\n",
    "'V335', 'V336', 'V337', 'V338', 'V339', 'V35', 'V36', 'V37', 'V38', 'V39', 'V4', 'V40', 'V41', 'V43', 'V44', 'V45', 'V46', 'V47', 'V49',\n",
    "'V5', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V58', 'V6', 'V60', 'V61', 'V62', 'V64', 'V65', 'V66', 'V67', 'V68', 'V7', 'V70', 'V72', \n",
    "'V73', 'V75', 'V76', 'V77', 'V78', 'V79', 'V8', 'V80', 'V82', 'V83', 'V85', 'V86', 'V87', 'V88', 'V9', 'V90', 'V93', 'V98']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Negative downsampling\n",
    "This approach is also good for current competition.\n",
    "The idea behind it - our model should find anomalies among normal transactions but reduced amount of normal transactions don't affect anomalies.\n",
    "\n",
    "It could speed up training process by 4-5 times without losing the quality. There is very low difference between sample (0.9489), full data (0.9496) and LB (0.9496) in my CV.\n",
    "\n",
    "It gives us ability to check many different hypotheses fast and to find best features within short time.\n",
    "When we find best features/parameters we could use full data to get maximum score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Approx. 99.8% of the data obey the following formula in train and test for variables V126 to V137\n",
    "V126 = V129 + V132 + V135\n",
    "V127 = V130 + V133 + V136\n",
    "V128 = V131 + V134 + V137\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
